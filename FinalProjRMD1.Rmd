---
title: "DS2 Final"
author: "M. de Ferrante, K. Maciejewski, P. Batten"
date: "April 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
```

###Intro
For our analysis we used Breast Cancer data from the Wisconsin Breast Cancer Database, compiled by Dr. William Wolberg MD, in the early 1990s, found in the `mlbench` R package. The objective of this dataset is to to use characteristics of tumors such as cell size and texture to classify the tumors as either benign or malignant. The goals of this analysis are to identify the best learning method for classifying tumors. 

Methods used include logistic regression, K-means clustering, Support Vector Machine, LDA, Random Forests, and KNN. While it will be helpful to identify one specific method for predicting future classifications of tumor, it is still advisable to consider the output from all methods, albeit with weighted considerations based on how effective the models are. Ultimately, while machine learning is a very powerful tool for making highly important medical classifications,  user discretion should be taken into account when considering interpretability, such as potentially using a simpler model that has nearly the same power as a more complex model. 


```{r, summary_all}
### Summary stuff for DS2 final project

library(mlbench)
data(BreastCancer)
attach(BreastCancer)
library(dplyr)
BreastCancer <- BreastCancer[,-1] # remove ID column
summary(BreastCancer) # note that everything is factor
```

Above, we load in the data and remove the ID column as it is not needed.

Each variable except Class is loaded as 11 numerical factors with values ranging from 0 through 10. Class is benign or malignant, and this is the variable of interest. There are 16 missing values in bare nuclei, as seen in the summary above.

The variables included in the dataset are Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses, and Class. There are 458 subjects with benign growths and 241 with malignant.

Below, we convert the factors into numeric values and remove the NA's. Since there are few missing values compared to the number in the dataset, removing them should not effect our overall analysis power.

```{r, data cleaning}
BreastCancer = BreastCancer %>% 
  mutate(Cl.thickness=as.numeric(Cl.thickness)) %>% 
  mutate(Cell.size=as.numeric(Cell.size)) %>%  
  mutate(Cell.shape=as.numeric(Cell.shape)) %>%  
  mutate(Marg.adhesion=as.numeric(Marg.adhesion)) %>%  
  mutate(Epith.c.size=as.numeric(Epith.c.size)) %>%  
  mutate(Bare.nuclei=as.numeric(Bare.nuclei)) %>%  
  mutate(Bl.cromatin=as.numeric(Bl.cromatin)) %>%  
  mutate(Normal.nucleoli=as.numeric(Normal.nucleoli)) %>%  
  mutate(Mitoses=as.numeric(Mitoses)) %>% na.omit()
```

### Summary plots

```{r ggpairs_num_plot}
library(GGally)
ggpairs(BreastCancer) # all but response are numeric
```


The correlation plot below does not give much information. As expected, most measures have correlation with the outcome. Mitoses, however, has almost no corellation.

```{r, cor_plot_wresp}
library(psych)
BreastCancer_num <- BreastCancer %>% 
  mutate(Class = as.numeric(Class)-1) # numeric response
cor.plot(BreastCancer_num[,])
```

Below are the density plots for the variables. The blue line signifies benign and pink are malignant. We see that there are few malignant subjects with the following: normal nuclei, mitoses, marginal adhesion, single epithelial cell size, uniformity of cell size, uniformity of cell shape. 

There are differences in the density plots of the following varaiables: bland chromatin, bare nucleoli, clump thickness.

The box plots below also show differences in the distributions for bland chromatin, bare nuclei, clump thickness.


```{r, try_feature_plots}
library(caret)
featurePlot(x=BreastCancer[,-10], y=BreastCancer[,10], 
            plot="density", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3),
            layout=c(3,3))

featurePlot(x=BreastCancer[,-10], y=BreastCancer[,10], 
            plot="box", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3),
            layout=c(3,3))
```

## Logistic Analysis

Here is a generalized linear model with all variables included.

```{r, glm_all}
glm1 = glm(Class ~., data=BreastCancer,family=binomial)

summary(glm1)
```

At $\alpha$ = 0.05 the following appear significant :

> Cl.thickness

> Marg.adhesion

> Bare.nuclei

> Bl.cromatin

We rerun the generalized linear model with only the significant values:

```{r, glm_sig}
glm2 = glm(Class ~ Cl.thickness + Marg.adhesion + 
             Bare.nuclei + Bl.cromatin, 
           data=BreastCancer,family=binomial)

summary(glm2)
```

(conclusion/ explanation here)

Below are the feature plots for only the significant predictors, so that we may better see the difference in distributions between the variable and the outcome.

```{r}
featurePlot(x=BreastCancer[,c(1,4,6,7)], y=BreastCancer[,10], 
            plot="density", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3))

featurePlot(x=BreastCancer[,c(1,4,6,7)], y=BreastCancer[,10], 
            plot="box", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3))
```

Now we split our data into a training and test set so we can make predictions

```{r}
set.seed(1)
BreastCancer.train <- sample(1:nrow(BreastCancer), 410) 

BreastCancer.test=BreastCancer[-BreastCancer.train,] # test

Class.test=BreastCancer$Class[-BreastCancer.train]

glm.fits=glm(Class ~ Cl.thickness + Marg.adhesion + 
             Bare.nuclei + Bl.cromatin, 
           data=BreastCancer,family=binomial, subset=BreastCancer.train)

glm.probs = predict(glm.fits, BreastCancer.test, type = "response")
glm.pred=rep("benign",273)
glm.pred[glm.probs >.5]="malignant"
table(glm.pred, Class.test)

confusionMatrix(glm.pred, Class.test, positive = "malignant")

mean(glm.pred == Class.test)

library(pROC)

roc.glm.train <- roc(BreastCancer.test$Class, glm.probs,
               levels = c("benign", "malignant"))
plot(roc.glm.train, legacy.axes = TRUE)

auc(roc.glm.train)
```

There were only 13 incorrect predictions; 5 benign were predicted to be malignant and 8 that were truly malignant were predicted to be benign. Logistic has 95% correct response for test, which is pretty good.

The area under the ROC curve is 99%.

Sensitivity is 91%

Specificity is 97%

PPV is 94% and NPV is 96%


