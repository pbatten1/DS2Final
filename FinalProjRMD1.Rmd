---
title: "DS2 Final"
author: "M. de Ferrante, K. Maciejewski, P. Batten"
date: "April 26, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE)
```

# Intro

For our analysis we used Breast Cancer data from the Wisconsin Breast Cancer Database, compiled by Dr. William Wolberg MD, in the early 1990s, found in the `mlbench` R package. The objective of this dataset is to to use characteristics of tumors such as cell size and texture to classify the tumors as either benign or malignant. The goals of this analysis are to identify the best learning method for classifying tumors. 

Methods used include logistic regression, K-means clustering, Support Vector Machine, LDA, Random Forests, and KNN. While it will be helpful to identify one specific method for predicting future classifications of tumor, it is still advisable to consider the output from all methods, albeit with weighted considerations based on how effective the models are. Ultimately, while machine learning is a very powerful tool for making highly important medical classifications,  user discretion should be taken into account when considering interpretability, such as potentially using a simpler model that has nearly the same power as a more complex model. Since our research area is highly regulated and in the clinical setting, we chose to use an untouched testing set to evaluate the performance of our methods. We also examine the kappa statistic as a measure of prediction accuracy since our outcome is unbalanced. 

# Data Cleaning

```{r, summary_all, include = FALSE}
### Summary stuff for DS2 final project

library(mlbench)
data(BreastCancer)
attach(BreastCancer)
library(dplyr)
BreastCancer <- BreastCancer[,-1] # remove ID column
summary(BreastCancer) # note that everything is factor
```

We load in the data and remove the ID column as it is not needed.

Each variable except Class is loaded as 11 numerical factors with values ranging from 0 through 10. Class is benign or malignant, and this is the variable of interest. There are 16 missing values in bare nuclei, as seen in the summary above.

The variables included in the dataset are Clump Thickness, Uniformity of Cell Size, Uniformity of Cell Shape, Marginal Adhesion, Single Epithelial Cell Size, Bare Nuclei, Bland Chromatin, Normal Nucleoli, Mitoses, and Class. There are 458 subjects with benign growths and 241 with malignant.

We convert the factors into numeric values and remove the NA's. Since there are few missing values compared to the number in the dataset, removing them should not effect our overall analysis power.

```{r, data cleaning, include = FALSE}
BreastCancer = BreastCancer %>% 
  mutate(Cl.thickness=as.numeric(Cl.thickness)) %>% 
  mutate(Cell.size=as.numeric(Cell.size)) %>%  
  mutate(Cell.shape=as.numeric(Cell.shape)) %>%  
  mutate(Marg.adhesion=as.numeric(Marg.adhesion)) %>%  
  mutate(Epith.c.size=as.numeric(Epith.c.size)) %>%  
  mutate(Bare.nuclei=as.numeric(Bare.nuclei)) %>%  
  mutate(Bl.cromatin=as.numeric(Bl.cromatin)) %>%  
  mutate(Normal.nucleoli=as.numeric(Normal.nucleoli)) %>%  
  mutate(Mitoses=as.numeric(Mitoses)) %>% na.omit()
```

# Exploratory data analysis

### Summary plots

The correlation plot below does not give much information. As expected, most measures have correlation with the outcome. Mitoses, however, has almost no corellation.

```{r, cor_plot_wresp, echo = FALSE}
library(psych)
BreastCancer_num <- BreastCancer %>% 
  mutate(Class = as.numeric(Class)-1) # numeric response
cor.plot(BreastCancer_num[,])
```

Below are the density plots for the variables. The blue line signifies benign and pink are malignant. We see that there are few malignant subjects with the following: normal nuclei, mitoses, marginal adhesion, single epithelial cell size, uniformity of cell size, uniformity of cell shape. 

There are differences in the density plots of the following varaiables: bland chromatin, bare nucleoli, clump thickness.

The box plots below also show differences in the distributions for bland chromatin, bare nuclei, clump thickness.


```{r, try_feature_plots, echo = FALSE}
library(caret)
featurePlot(x=BreastCancer[,-10], y=BreastCancer[,10], 
            plot="density", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3),
            layout=c(3,3))

featurePlot(x=BreastCancer[,-10], y=BreastCancer[,10], 
            plot="box", 
            scales=list(x=list(relation="free"), 
                        y=list(relation="free")), 
            auto.key=list(columns=3),
            layout=c(3,3))
```

# Supervised Analyses

## Logistic Analysis

When we use a generalized linear model with all variables included, at $\alpha$ = 0.05 the following appear significant: `Cl.thickness`, `Marg.adhesion`, `Bare.nuclei`,
`Bl.cromatin`

```{r, glm_all, eval = FALSE}
glm1 = glm(Class ~., data=BreastCancer,family=binomial)

summary(glm1)
```


Now we split our data into a training and test set so we can make predictions

```{r logistic_training_testing_sets, include = FALSE}
set.seed(1)
BreastCancer.train <- sample(1:nrow(BreastCancer), 410) 

BreastCancer.test=BreastCancer[-BreastCancer.train,] # test

Class.train=BreastCancer$Class[BreastCancer.train]

Class.test=BreastCancer$Class[-BreastCancer.train]

glm.fits=glm(Class ~ Cl.thickness + Marg.adhesion + 
             Bare.nuclei + Bl.cromatin, 
           data=BreastCancer,family=binomial, subset=BreastCancer.train)

glm.probs = predict(glm.fits)
glm.pred=rep("benign",410)
glm.pred[glm.probs >.5]="malignant"

train <- confusionMatrix(glm.pred, Class.train, positive = "malignant")

glm.probs = predict(glm.fits, BreastCancer.test, type = "response")
glm.pred=rep("benign",273)
glm.pred[glm.probs >.5]="malignant"

test <- confusionMatrix(glm.pred, Class.test, positive = "malignant")
```


```{r}
train

test
```

There were only 13 incorrect predictions; 5 benign were predicted to be malignant and 8 that were truly malignant were predicted to be benign. Logistic has 95% correct response for test, which is pretty good.

The area under the ROC curve is 99%.

Sensitivity is 91%

Specificity is 97%

PPV is 94% and NPV is 96%

## LDA and QDA

```{r lda, message=FALSE, warning=FALSE, include=FALSE}
library(MASS)
library(pROC)
library(ISLR)
library(caret)

set.seed(1)
train_index <- sample(1:683, 410)

train <- BreastCancer[train_index,]
test <- BreastCancer[-train_index,]

lda.fit <- lda(Class ~ ., data = train)
plot(lda.fit)
lda.pred <- predict(lda.fit, newdata = test)

roc.lda <- roc(test$Class, lda.pred$posterior[,2], 
               levels = c("benign", "malignant"))
#plot(roc.lda, legacy.axes = TRUE)



confusionMatrix(lda.pred$class, test$Class, positive = "malignant")
diag(prop.table(table(lda.pred$class, test$Class), 1))


set.seed(1)
qda.fit <- qda(Class ~ ., data = train)

qda.pred <- predict(qda.fit, newdata = test)
confusionMatrix(qda.pred$class, test$Class, positive = "malignant")
diag(prop.table(table(qda.pred$class, test$Class), 1))
```

Linear Discriminant Analysis (LDA) resulted in 95.6% accuracy and a kappa statistic of .9 when comparing predictions from the model generated with the training data to the test data classes. Additionally sensitivity was 90.22% and specificity was 98.34% for LDA. For Quadratic Discriminant Analysis (QDA), there was also 95.6% prediction accuracy for the test data and the kappa statistic was slightly improved with a statistic of .9032. QDA had sensitivity of 96.74% and specificity of 95.03%. With higher sensitivity, we have a higher probability of the model accurately predicting malignancy (versus a tumor being benign) given that someone has a malignant tumor. This is important for being able to treat patients using chemotherapy or performing surgery if necessary. With high specificity, we have a higher probability of predicting that someone has a benign tumor given that they have a benign tumor. This is important because we do not want to subject patients to unnecessary risk by performing unnecessary surgery, or putting them on chemotherapy if they don't need to be since it can be very damaging to a patients health. Since both of these values are extremely important for patient health and safety, QDA would be a better choice for a model since we sacrifice too much in sensitivity with LDA. QDA ends up maximizing these values almost equally.


## KNN

#### Choosing Number of Neighbors
```{r knn, message=FALSE, warning=FALSE, include=FALSE}
library(class)
set.seed(1)

area_under_curve <- rep(0, 10)
accuracy <- rep(0, 10)
for(i in 1:10) {
  set.seed(1)
  knn_pred <- knn(train[,-10], test[,-10], train$Class, k = i, prob = TRUE)
  scores.knn <- attr(knn_pred,"prob")
  knnROC <- roc(test$Class, scores.knn, 
               levels = c("benign", "malignant"))
  
  area_under_curve[i] <- auc(knnROC)
  accuracy[i] <- confusionMatrix(knn_pred, test$Class)$overall[1]
}



knn_output <- data_frame(1:10, area_under_curve, accuracy)
colnames(knn_output) <- c("Number of Neighbors", "Area Under Curve", "Accuracy")

kable(knn_output, align = "c")
```

```{r}
plot(1:10, area_under_curve, xlab = "Number of Neighbors", ylab = "Area Under Curve")

plot(1:10, accuracy, xlab = "Number of Neighbors", ylab = " Test Accuracy")

```

```{r knn2, message=FALSE, warning=FALSE, include=FALSE}
set.seed(1)
knn_pred <- knn(train[,-10], test[,-10], train$Class, k = 3, prob = TRUE)
scores.knn <- attr(knn_pred,"prob")
knnROC <- roc(test$Class, scores.knn, 
              levels = c("benign", "malignant"))
  
auc(knnROC)
```

### 

```{r}
confusionMatrix(knn_pred, test$Class, positive = "malignant")
```

After comparing the area under the ROC curve and the percentage of acccuracy in predicting tumor type, the best number of neighbors was chosen to be 3. This value appears to maximize accuracy without losing too much in area under the curve. The chosen number of neighbors has area under the curve equal to .4378 and prediction accuracy against the test set of 96.7%. The kappa statistic is .9256. The sensitivity and specificity are 93.48% and 98.34%, respectively. 


## Support Vector Machine 
#### Linear Kernel

```{r svm, message=FALSE, warning=FALSE, include=FALSE}
library(e1071)

set.seed(1)
tune.out <- e1071::tune(svm, Class ~ ., data = train, 
                 type = "C-classification", kernel = "linear", 
                 ranges = list(cost = c(0.001 , 0.01, 0.1, 1,5,10,100)))

svmfit <- e1071::svm(Class ~ ., data = train, kernel = "linear", cost = 0.01)

pred.svm.train <- predict(svmfit, test)
```


```{r}
confusionMatrix(test$Class, pred.svm.train, positive = "malignant")
```

#### Radial Kernel

```{r svm2, message=FALSE, warning=FALSE, include=FALSE}
tune.out <- tune(svm, Class ~ ., data = train, 
                 kernel = "radial",
                 ranges = list(cost = c(0.1,1,10,100,1000),
                          gamma = c(0.0001,0.001,0.01,0.1,0.5,1)))

svmfit <- e1071::svm(Class ~ ., data = train, kernel = "radial", cost = 10, gamma = .1)

svm.tune.pred.train <- predict(svmfit, 
                         newdata = test)
```

```{r}
confusionMatrix(test$Class, svm.tune.pred.train, positive = "malignant")

```

The linear kernel with tuning resulted in better performance using cost = .01, and with this model the prediction accuracy on the test data was 95.6% with a kappa statistic of .9011. The sensitivity for the linear kernel was 94.44% and the specificity was 96.17%. Support vector classification with a radial kernel had test set accuracy of 94.87% and a kappa statistic of .8858 (after tuning the parameters). The sensitivity for the radial kernel was 91.49% and the specificity was 96.65%. In comparing prediction accuracy of support vector classification using a linear kernel versus a radial kernel after tuning parameters, the linear kernel performed better on this data. 

## Random Forest

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(mlbench)
data(BreastCancer)
library(tidyverse)
library(randomForest)
library(caret)
```


```{r training/test sets, message=FALSE, warning=FALSE, include=FALSE}
set.seed(1)
train_sample <- sample(1:nrow(BreastCancer), 410) 
BreastCancer.train=BreastCancer[train_sample,]
BreastCancer.test=BreastCancer[-train_sample,]
```

```{r random forests, message=FALSE, warning=FALSE, include=FALSE}
set.seed(10)
BC.rf=randomForest(Class ~ . , data = BreastCancer, subset = train_sample, na.action = na.exclude, importance=T)
BC.rf #RF chooses 3 variables
plot(BC.rf)
summary(BC.rf)
importance(BC.rf, type=1)

test.err=double(10)
oob.err=double(10)
for(mtry in 1:10) 
{
  rf=randomForest(Class ~ . , data = BreastCancer.train,
                  mtry=mtry, ntree=200, na.action = na.exclude) 
  oob.err[mtry] = rf$err.rate[200]
  
  pred=predict(rf, BreastCancer.test[1:10]) #Predictions on Test Set for each Tree
  test.err[mtry]= mean(((as.numeric(BreastCancer.test$Class)-1) - (as.numeric(pred)-1)^2),
                       na.rm = T) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}
oob.err
test.err
```

```{r Output, echo=TRUE}
confusionMatrix(pred, BreastCancer.test$Class, positive = 'malignant')
importance(rf, type = 2)
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("bottomright",legend=c("Out of Bag Error","Test Error"), pch=19, col=c("red","blue"))
```

Applying random forests techniques to predict tumor classification results in 3 variables being deemed important in contributing to this classification, as suggested by the Oob error being minimized at 3 predictors and the relatively low test error at 3 predictors. Restricting the number of variables used at each split per tree to three yields the best out-of-bag error, as well as the (tied-for) best test error. 3 variables is also relatively simple, which is important to consider when building classifier models. The 3 variables that most effectively decrease impurity at each decision node are cell size, bland chromatin (which measures texture of thhe nucleus), and bare nuclei, a term used to describe the composition of the nucleus. The accuracy of the random forests in predicitng tumor type are as follows: Accuracy = 95.45%, Kappa statistic = 0.9012, Sensitivity = 0.9231, Specificity = 0.9725 (where sensitivity predicts Malignant tumors). Comparisons to other learning methods can be seen in the table in the Conclusion section.


# Conclusions 

## Overall Comparison of Methods

```{r}
library(knitr)

accuracy <- c("95.24%" , "95.45%", '96.7%', "95.6%", "95.6%", "95.6%")
kappa_stat <- c(.8926 , .9012, .9256, .9, .9032, .9011)
sensitivity <- c("91.30%", '92.31%', "93.48%", "90.22%", "96.74%", "94.4%")
specificity <- c("97.24%", "97.25%", "98.34%", "98.34%", "95.03%", "96.17%")


summary <- data_frame(accuracy, kappa_stat, sensitivity, specificity)

row.names(summary) <- c("Logistic Regression", "Random Forest", "KNN","LDA","QDA", "Support Vector Classification")

colnames(summary) <- c("Test Accuracy", "Kappa Statistic", "Sensitivity", "Specificity")

kable(summary, align = "c")
```

