---
title: "PeterRMD"
author: "M. de Ferrante, K. Maciejewski, P. Batten"
date: "April 26, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(mlbench)
data(BreastCancer)
library(tidyverse)
library(randomForest)
library(caret)
```


```{r training/test sets, message=FALSE, warning=FALSE, include=FALSE}
set.seed(1)
train_sample <- sample(1:nrow(BreastCancer), 410) 
BreastCancer.train=BreastCancer[train_sample,]
BreastCancer.test=BreastCancer[-train_sample,]
```

```{r random forests, message=FALSE, warning=FALSE, include=FALSE}
set.seed(10)
BC.rf=randomForest(Class ~ . , data = BreastCancer, subset = train_sample, na.action = na.exclude, importance=T)
BC.rf #RF chooses 3 variables
plot(BC.rf)
summary(BC.rf)
importance(BC.rf, type=1)

test.err=double(10)
oob.err=double(10)
for(mtry in 1:10) 
{
  rf=randomForest(Class ~ . , data = BreastCancer.train,
                  mtry=mtry, ntree=200, na.action = na.exclude) 
  oob.err[mtry] = rf$err.rate[200]
  
  pred=predict(rf, BreastCancer.test[1:10]) #Predictions on Test Set for each Tree
  test.err[mtry]= mean(((as.numeric(BreastCancer.test$Class)-1) - (as.numeric(pred)-1)^2),
                       na.rm = T) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
  
}
oob.err
test.err
```

```{r Output, echo=TRUE}
confusionMatrix(pred, BreastCancer.test$Class, positive = 'malignant')
importance(rf, type = 2)
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split")
legend("bottomright",legend=c("Out of Bag Error","Test Error"), pch=19, col=c("red","blue"))
```

Applying random forests techniques to predict tumor classification results in 3 variables being deemed important in contributing to this classification, as suggested by the Oob error being minimized at 3 predictors and the relatively low test error at 3 predictors. Restricting the number of variables used at each split per tree to three yields the best out-of-bag error, as well as the (tied-for) best test error. 3 variables is also relatively simple, which is important to consider when building classifier models. The 3 variables that most effectively decrease impurity at each decision node are cell size, bland chromatin (which measures texture of thhe nucleus), and bare nuclei, a term used to describe the composition of the nucleus. The accuracy of the random forests in predicitng tumor type are as follows: Accuracy = 95.45%, Kappa statistic = 0.9012, Sensitivity = 0.9231, Specificity = 0.9725 (where sensitivity predicts Malignant tumors). Comparisons to other learning methods can be seen in the table in the Conclusion section. 


>Write-up

###Intro
For our analysis we used Breast Cancer data from the Wisconsin Breast Cancer Database, compiled by Dr. William Wolberg MD, in the early 1990s, found in the "mlbench" R package. The objective of this dataset is to to use characteristics of tumors such as cell size and texture to classify the tumors as either benign or malignant. The goals of this analysis are to identify the best learning method for classifying  tumors. 
Methods used include logistic regression, K-means clustering, Support Vector Machine, LDA, Random Forests, and KNN. While it will be helpful to identify one specific method for predicting future classifications of tumor, it is still advisable to consider the output from all methods, albeit with weighted considerations based on how effective the models are. Ultimately, while machine learning is a very powerful tool for making highly important medical classifications,  user discretion should be taken into account when considering interpretability, such as potentially using a simpler model that nearly the same power as a more complex model. 
